---
title: "Hadoop Streaming and R"
author: "Darrell Aucoin"
date: "January 4, 2015"
output: html_document
---

# RHadoop (rmr2 package)

For any R package to work with hadoop, it must be installed on all clusters with the exact same location path.

Many rmr2 function will return something called a __Big Data Object__, a stub with some information on finding and managing data (the data itself is not loaded into memory until needed). Some functions in rmr2 that return __Big Data Objects__ are mapreduce, to.dfs, and from.dfs.

We can switch off hadoop, for debugging purposes with `rmr.options`.

Information gathered here come from the rmr2 help function and the [rmr2 tutorial](https://github.com/RevolutionAnalytics/rmr2/blob/master/docs/tutorial.md)

## rmr2 Functions
```R
mapreduce(input, output = NULL, map = to.map(identity), reduce = NULL,
  vectorized.reduce = FALSE, combine = NULL, in.memory.combine = FALSE,
	input.format = "native",   output.format = "native",
	backend.parameters = list(),   verbose = TRUE)
```

__mapreduce__ Defines and executes a map reduce job. (Pkg rmr2)

- __input__ Can be:

    - A set of file paths in HDFS

    - A Big Data Object (a stub of information on some data in HDFS)

    - A list of a combination of both

- __output__ A path to the destination folder on HDFS; if missing, a __Big Data Object__ is returned.

- __map__ An optional R function of two arguments, returning either NULL or the return value of keyval, that specifies the map operation to execute as part of a mapreduce job. The two arguments represent multiple key-value pairs according to the definition of the mapreduce model. 

    - The map function must return key-value pairs. Preferably using the function keyval:  
    `return( keyval(key, val) )` where key, val can be vectors, lists, matricies, data.frames and val can even be NULL

    - Keys are matched to the corresponding values by position, according to the second dimension if it is defined (that is rows in matrices and data frames, position otherwise), analogous to the behavior of cbind, see keyval for details.

- __reduce__ An optional R function of two arguments, a key and a data structure representing all the values associated with that key (the same type as returned by the map call, merged with rbind for matrices and data frames and c otherwise), returning either NULL or the return value of `keyval`, that specifies the reduce operation to execute as part of a mapreduce job. 
    - The default is no reduce phase, that is the output of the map phase is the output of the mapreduce job, see the vectorized.reduce argument for an alternate interface

- __vectorized.reduce__ The argument to the reduce should be construed as a collection of keys and values associated to them by position (by row when 2-dimensional). Identical keys are consecutive and once a key is present once, all the records associated with that key will be passed to the same reduce call (complete group guarantee). This form of reduce has been introduced mostly for efficiency reasons when processing small reduce groups, because the records are small and few of them are associated with the same key. This option affects the combiner too.

- __combine__ refers to:

    - A function with the same signature and possible return values as the reduce function, or 

    - TRUE, which means use the reduce function as combiner.

    - NULL means no combiner is used.

- __in.memory.combine__ Apply the combiner just after calling the map function, before returning the results to hadoop. This is useful to reduce the amount of I/O and (de)serialization work when combining on small sets of records has any effect (you may want to tune the input format to read more data for each map call together with this approach, see arguments read.size or nrow for a variety of formats)

- __input.format__ Input format specification, see make.input.format  

$$latex
\mbox{input.format}=\begin{cases}
\mbox{"text"} & \mbox{Plain text}\\
\mbox{"json"} & \mbox{JavaScript Object Notation}\\
\mbox{"csv"} & \mbox{Comma Separated Values}\\
\mbox{"native"}\\
\mbox{"sequence.typedbytes"}\\
\mbox{"hbase"} & \mbox{hbase table format}\\
\mbox{"pig.hive"} & \mbox{pig/hive table format (some similarities to csv)}
\end{cases}
$$

$$
\int
$$
