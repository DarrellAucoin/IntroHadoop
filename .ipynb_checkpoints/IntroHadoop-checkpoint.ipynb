{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:d8023bcefd6ac2e08d69002ec8f743408e4249c6a01ca4e03dfe870307b358ef"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<head>\n",
      "<script type=\"text/javascript\"\n",
      "  src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\">\n",
      "</script>\n",
      "\n",
      "</head>\n",
      "\n",
      "<style>\n",
      "\n",
      "@font-face {\n",
      "    font-family: \"Computer Modern\";\n",
      "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
      "}\n",
      "#notebook_panel { /* main background */\n",
      "    background: #888;\n",
      "    color: #f6f6f6;\n",
      "}\n",
      "#notebook li { /* More space between bullet points */\n",
      "margin-top:0.8em;\n",
      "}\n",
      "div.text_cell_render{\n",
      "    font-family: 'Arvo' sans-serif;\n",
      "    line-height: 130%;\n",
      "    font-size: 135%;\n",
      "    width:1000px;\n",
      "    margin-left:auto;\n",
      "    margin-right:auto;\n",
      "}\n",
      "\n",
      "</style>\n",
      "\n",
      "\n",
      "<center>\n",
      "\n",
      "<p class=\"gap05\"<p>\n",
      "<h1>Introducton to ![alt text](Images/hadoop-logo.jpg)</h1>\n",
      "\n",
      "<p class=\"gap05\"<p>\n",
      "<h3>Darrell Aucoin</h3>\n",
      "\n",
      "<h3>Stats Club</h3>\n",
      "\n",
      "<p class=\"gap2\"<p>\n",
      "</center>\n",
      "> In pioneer days they used oxen for heavy pulling, and when one ox couldn\u2019t budge a log, they didn\u2019t try to grow a larger ox. We shouldn\u2019t be trying for bigger computers, but for more systems of computers.  \n",
      "> \u2014Grace Hoppe\n",
      "\n",
      "<style type=\"text/css\">\n",
      ".input_prompt, .input_area, .output_prompt {\n",
      "    display:none !important;\n",
      "}\n",
      "</style>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Motivation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Q:__ Why do we need Hadoop when we have SQL?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Our capability of storing data has outpaced our ability to read it\n",
      "    - Harddrives speeds have not improved as fast as their capacity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Processing capacity have bottlenecked, increasing the need for parallelization of tasks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- SQL servers do not scale very well in comparison to Hadoop\n",
      "    - SQL is best for Gigabytes of data, Hadoop for Terabyes/Petrabyes of data\n",
      "    - SQL skills are still useful in a Hadoop ecosystem (Hive, Spark SQL, Impala)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "__Q:__ Who uses Hadoop?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "HTML('<iframe src=http://wiki.apache.org/hadoop/PoweredBy#A width=1000 height=600></iframe>')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "html": [
        "<iframe src=http://wiki.apache.org/hadoop/PoweredBy#A width=1000 height=600></iframe>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "<IPython.core.display.HTML at 0x103879990>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "__Q:__ Why learn Hadoop?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- According to [O'Reilly Data Science Survey](http://www.oreilly.com/data/free/2014-data-science-salary-survey.csp), knowledge of Hadoop and related tools are correlated to the highest salaries in Data Science\n",
      "    - The highest salaries are correlated to joint knowledge of Hadoop and SQL"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Hadoop vs RDBMS (SQL)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "| | RDBMS (SQL) | Hadoop |\n",
      "|-|-------------|--------|\n",
      "| Data Size | Gigabytes | Terabyte/Petrabytes |\n",
      "| Access | Interactive and batch | Batch |\n",
      "| Updates | Read and write many times | Write once, read many times |\n",
      "| Structure | Static schema (highly structured) | Dynamic schema (Semi-structured) |\n",
      "| Integrity | High | Low |\n",
      "| Scaling | Nonlinear | Linear |\n",
      "| Schema | Write-on Schema | Read-on Schema |\n",
      "| Data formating | Normalized | Optimally De-normalized |"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "When going from a RDBMS to Hadoop, the biggest trade off is the guanrantee of atomicity, consistency, isolation, and durability for scalability."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "__Write-on\u00a0Schema__: Information is inputted, transformed and written into the predefined schema: we can enforce consistency through this. \n",
      "\n",
      "__Read-on\u00a0Schema__: Bring in files without any predefined gatekeeping or consistency services. The schema is created when reading the files using a set of predefined rules. For instance, a comma seperated file could have it's first entry interpretated as a string, the second as an integer, the third as a float, etc."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What is Hadoop?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Definition.__ __Hadoop__ is a MapReduce framework built on top of a distributed file system (HDFS for Hadoop). "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Hadoop is distributed system, meaning it's an interconnected system of computers that looks to the user as just one single entity.\n",
      "<center>\n",
      "![alt text](Images/Distributed_Hadoop.png)\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "- When a client submits a MapReduce Job, the job is split into several tasks and the nodes takes on a fixed number of tasks per node\n",
      "- Written in mainly in Java, with some components written in C and bash\n",
      "- Designed around processing high throughput of large data rather than response times  \n",
      "- Hadoop is highly configurable, both in it's operations and job configurations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "__Note:__\n",
      "\n",
      "- All components and modules have to be installed on every node in a cluster (i.e. to use a module in R, the module has to be installed on every node)\n",
      "- A single node may be running several tasks from several different jobs"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Hadoop V1.x and V2.x"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Due to compatiability breaking problems with V1.x, V2.x was created:\n",
      "- 2.x allows Hadoop Streaming to stream binary data compared to just text with 1.x\n",
      "- 2.x is more scalable, 1.x has a bottleneck in it's design (a singular NameNode)\n",
      "- Various Hadoop configuration options changed names"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "What is MapReduce?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Definition.__ _MapReduce_ is a programming paradigm model of using parallel, distributed algorithims to process or generate data sets. MapRedeuce is composed of two main functions:\n",
      "\n",
      "__Map(k,v)__: Filters and sorts data.\n",
      "\n",
      "__Reduce(k,v)__: Aggregates data according to keys (k)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Hadoop MapReduce Components"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Hadoop 1.x\n",
      "__JobTracker__ Coordinates jobs, scheduling task for tasktrackers and records progress for each job\n",
      "- If a task fails, it's rescheduled on different TaskTracker\n",
      "\n",
      "__TaskTracker__ Runs tasks and send progress reports to the jobtracker"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "###Hadoop 2.x\n",
      "__ResourceManager__: Arbitrates resources among all applications in the system.\n",
      "\n",
      "__ApplicationMaster__: A per-application framework tasked with negotiating resources from the ResourceManager and working with the NodeManager(s) to execute and manitor tasks the component tasks.\n",
      "\n",
      "__MRAppMaster__: per application\n",
      "\n",
      "__NodeManager__: YARN's per-node agent: \n",
      "- Keeping up-to-date with ResourceManager\n",
      "- Overseeing individual tasks life-cycle management\n",
      "    \u2013 Constructing a JVM, running the task, then deconstructing the JVM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "- Monitoring resource usage for each task: memory, CPU\n",
      "- Tracking node-health\n",
      "- Logging management and auxiliary services "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "What is HDFS?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Definition.__ __HDFS (Hadoop Distributed File System)__ is a fault tolerant, distributed, scalable file-system accross multiple interconnected computer systems (nodes). "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "__Fault\u00a0tolerant__ means that a single node failure will not halt operations. It does this by replicating the data accross multiple nodes (usually 3)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "__Distributed__ means that data is shared accross multiple computer nodes. This means that one node can have data that another node does not. The data nodes can \u201ctalk\u201d to each other to balance the data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "__Scalable__ means that performance for clients/node are roughly linear. To increase performance, add new nodes which allows a greater number of clients."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "HDFS Components"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__NameNode__ The master node: maintains the namespace of the directories and files and manages the blocks on each DataNode. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "- Usually the most RAM intensive component of HDFS (keeping all metadata of files/directories in memory)  \n",
      "    - Best to have input files at least a gigabyte in size\n",
      "    - The greatest bottleneck for Hadoop 1.x, having only 1 NameNode: too many files requires more ram than the NameNode can provide. Hadoop 2.x uses HDFS Federation, a collection of independent NameNodes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "__DataNode__ Provides the actual storage of the blocks of files."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "__Secondary\u00a0NameNode__ Performs periodic checkpoints of the NameNode and in the event of NameNode failure, can restart the NameNode from last checkpoint (can take a while and a chance of data loss).\n",
      "- Secondary Namespace lags behind primary, thus total failure of primary will likely result in data loss\n",
      "- Otherwise, copy metadata to secondary and run that as the new primary"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Various Hadoop Projects"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[__Hive__](https://hive.apache.org/): Allows users to create SQL-like queries (HQL) and convert them to MapReduce jobs. (`INSERT, UPDATE, DELETE` statements are not allowed)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "[__Pig__](http://pig.apache.org/): An easy to learn hadoop-based language that is adept at very deep, very long data pipelines. Very commonly used with DataFu, LinkedIn assortment of User Defined Functions useful for data analysis."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "[__Spark__](https://spark.apache.org/): Another implementation of MapReduce that is more effective for iterative algorithms (machine learning algorithms). Also has built in packages for Spark SQL, and machine learning."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "[__HBase__](http://hbase.apache.org/): Non-relational database allowing low-latency, quick lookups in Hadoop. Can do updates, inserts, and deletes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "[__Oozie__](http://oozie.apache.org/): A workflow processing that lets users define a series of jobs written in multiple languages: Hadoop, Pig, and Hive and then link them to one another."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "[__Avro__](http://avro.apache.org/): A data serialization system that allows for encoding the schema of Hadoop files. It is adept at parsing data and performing removed procedure calls."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "[__Mahout__](http://mahout.apache.org/): A data mining library using the most popular data mining algorithims for performing clustering, regression testing and statistical modeling and implements them using the Map Reduce model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "[__Sqoop__](http://sqoop.apache.org/): A conectivity tool for moving data from non-Hadoop databases (SQL, etc.) into Hadoop."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "[__HCatalog__](http://hortonworks.com/hadoop/hcatalog/): A centralized metadata mangagement and sharing service for Hadoop, allowing a unified view of all data in Hadoop clusters."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "MapReduce Phases"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "MapReduce is broken down into several steps:\n",
      "\n",
      "1. Record Reader\n",
      "2. Map\n",
      "3. Combiner (Optional)\n",
      "4. Partitioner\n",
      "5. Shuffle and Sort\n",
      "6. Reduce\n",
      "7. Output Format"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Record Reader"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Record\u00a0Reader__ Translates an input into records to be processed by the user-defined map function in the form of a key-value pair on each map cluster. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "- The key is positional information (the number of bytes from start of file) and the value is the chunk of data composing a single record."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![alt text](images/MapReduceInput.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "- In hadoop, each map task's is an input split which is usually simply a HDFS block\n",
      "    - Hadoop tries scheduling map tasks on nodes where that block is stored (data locality)\n",
      "    - If a file is broken mid-record in a block, hadoop requests the additional information from the next block in the series"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Map"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Map__ _User defined function_ outputing intermediate key-value pairs for the reducers\n",
      "![alt text](images/MapReduceMapper.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "__key__\u00a0($k_{2}$): Later, MapReduce will group and possibly aggregate data according to these keys, choosing the right keys is here is important for a good MapReduce job.\n",
      "\n",
      "__value__\u00a0($v_{2}$): The data to be grouped according to it's keys."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Combiner (Optional)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Combiner__ _User defined function_ that aggregates data according to intermediate keys on a mapper node\n",
      "![alt text](images/MapReduceCombiner.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "- This can usually reduce the amount of data to be sent over the network increasing efficiency  \n",
      "\n",
      "$$\\left.\\begin{array}{r}\n",
      "\\left(\\mbox{\"hello world\"},1\\right)\\\\\n",
      "\\left(\\mbox{\"hello world\"},1\\right)\\\\\n",
      "\\left(\\mbox{\"hello world\"},1\\right)\n",
      "\\end{array}\\right\\} \\overset{\\mbox{combiner}}{\\longrightarrow}\\left(\\mbox{\"hello world\"},3\\right) $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Combiner should be written with the idea that it is executed over most but not all map tasks. ie. $$\\left(k_{2},v_{2}\\right)\\mapsto\\left(k_{2},v_{2}\\right)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Usually very similar or the same code as the reduce method."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Partitioner"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Partitioner__ Sends intermediate key-value pairs (k,v) to reducer by   $$\\mbox{Reducer}=\\mbox{hash}\\left(\\mbox{k}\\right)\\pmod{R}$$  \n",
      "![alt text](images/MapReducePartitioner.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "- will usually result in a roughly balanced load accross the reducers while ensuring that all key-value pairs are grouped by their key on a single reducer. \n",
      "- A balancer system is in place for the cases when the key-values are too unevenly distributed.\n",
      "- In hadoop, the intermediate keys ($k_{2},v_{2}$) are written to the local harddrive and grouped by which reduce they will be sent to and their key."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Shuffle and Sort"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Shuffle\u00a0and\u00a0Sort__ On reducer node, sorts by key to help group equivalent keys\n",
      "![alt text](images/MapReduceSort.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Reduce"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Reduce__ _User Defined Function_ that aggregates data (v) according to keys (k) to send key-value pairs to output\n",
      "![alt text](images/MapReduceReduce.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Output Format"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Output\u00a0Format__ Translates final key-value pairs to file format (tab-seperated by default).\n",
      "![alt text](images/MapReduceOutput.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "MapReduce Example: Word Count"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![alt text](http://xiaochongzhang.me/blog/wp-content/uploads/2013/05/MapReduce_Work_Structure.png)  \n",
      "Image Source: [Xiaochong Zhang's Blog](http://xiaochongzhang.me/blog/)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "HDFS Operation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__HDFS (Hadoop File System)__: A distributed file system designed to store huge streamable files running on commodity hardware."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "__Distributed\u00a0File\u00a0System__: A file system that manages and stores files across a network of computer allowing files to be parititioned and stored across multiple computer nodes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "__Community\u00a0Hardware__: Commonly available hardware. i.e. the computer in front of you."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "__Huge\u00a0Files__: File sizes from 100MB to petabytes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "__Streamable__: Documents in HDFS must be able to be partitioned where each partition can be read independently."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "- A LaTeX document is __NOT__ streamable as the begining of the document defines parameters that are used throught the whole document\n",
      "- A comma seperated value (CSV) text document __IS__ streamable as it is readable from any parition of the document"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### HDFS\u00a0is\u00a0not\u00a0very\u00a0good\u00a0with:\n",
      "\n",
      "1. __Low-latency requirements__: HDFS is designed with high throughput in mind, usually giving up low-latency access for higher throughput in the design decisions.\n",
      "\n",
      "2. __Lots of small files__: The namenode holds the filesystem metadata in main memory, meaning more files in the system means more memory needed in the namenode. Too many files can slow down  the cluster or create a system crash.\n",
      "\n",
      "    - Best to either concatonate files together or compress the files into a compression format that is partitionable (each HDFS block is independently readable)\n",
      "\n",
      "    - YARN (MapReduce 2) has less problems with lots of small files but is a problem\n",
      "\n",
      "3. __Arbitrary file modifications__: Files in HDFS can only be written to by a single writer by appending to the end of the file. The file can only be changed at the end as otherwise it would have to shift all of the other bits in the file (including other blocks) around."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "HDFS Concepts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__HDFS\u00a0Block__: Data stored in HDFS is broken into chucks, 64 MB by default, but 128 MB or 256MB are commonly used, which are spread accross various nodes in HDFS.\n",
      "- HDFS blocks are best utitilized by having them evenly distributed across the cluster and sufficiently large to justify creating a JVM for each map task.\n",
      "\n",
      "- The block size is a balance of reduction of head seeks on disk, RAM memory usage of bringing in blocks for map tasks, and distributing the blocks of the file across the cluster\n",
      "\n",
      "- A file smaller than the block will not occupy a full block's worth of underlying storage\n",
      "\n",
      "- Filesystem maintaince tools fsck can be used to work on blocks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "__HDFS\u00a0Federation__: For YARN (Hadoop 2.x) allows a cluster to have multiple namenodes which manages mutually exclusive potions of the filesystem namespace.\n",
      "\n",
      "- One NameNode might manage files/directories under /user and another manage /share\n",
      "\n",
      "- Manages a namespace volume: metadata of namespace, block pool (all blocks for files in namespace).\n",
      "\n",
      "- They are independent: if one NameNode fails, the others will still work just fine."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "__Balancer__: HDFS data is not always distributed evenly across the cluster, the balancer moves blocks across the cluster to create a rough balance.\n",
      "\n",
      "- Keep one of the replicas of a block on the same node that it is writing the block\n",
      "\n",
      "- One of the replicas is placed in the same rack as IO is preformed\n",
      "\n",
      "- Spread different replicas across racks to ensure no data loss with loss of a rack (one replica on a different rack)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Failure Recovery"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are several ways that a MapReduce job may fail. Failure recovery is thus built into Hadoop:\n",
      "\n",
      "1. A task failing.\n",
      "    1. Exception is thrown. i.e. A corrupt block being read.\n",
      "    2. Node running out of memory.\n",
      "    3. A task operating too long without a status update.\n",
      "2. A node failing (going offline).\n",
      "3. A NameNode failing (the worst point of failure)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Task Failing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "If a map or reduce throws a exeception and it isn't caught in code, then the task fails."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "By default, if a map task fails, then it is first rerun on the same node then tried one last time on a different node.\n",
      "\n",
      "- By default, if a task fails 3 times then the job fails."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Possible Causes of exceptions:\n",
      "- Reading a corrupt block \n",
      "  - Handle the exception in code (catch the exception)\n",
      "  - A bad record can be skipped by enabling skipping mode using `SkipBadRecords` class"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Bad formatting of the file\n",
      "    - Ensure correct formatting of the file\n",
      "    - Handle problems in code"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Node running out of memory\n",
      "    - Write memory efficient code"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Lack of Updates"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "After 10 mins, by default, of no updates from a task Hadoop tries to kill the task and rerun the task while running procedures to ensure consistency.\n",
      "\n",
      "- Ensure regular updates: either updating a counter, or writing to stderr (which is then sent to the client)\n",
      "    - This is really important for monte carlo simulations on hadoop."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Node Failing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If a DataNode fails, Hadoop tries isolating the node from the system and reassigning tasks that DataNode was operating on."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "NameNode Failing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If a NameNode fails then the whole system (or at least a partition of it for 2.x) is down. The system recovers by setting up a new NameNode via the information in the Secondary NameNode as well as the edit logs. However since there is a lag between the NameNode's operation and edit logs there is almost always __data loss__ after recovery."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- One cause of NameNode failure is too many files in the system:\n",
      "    - The NameNode keeps all metadata for HDFS files in memory, more files means more memory is needed and NameNode failing due to lack of memory.\n",
      "    - It is __HIGHLY__ recommended to keep most if not all data for a MapReduce job in a single file"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "With Hadoop 1.x can take about 30 mins for a new NameNode to be setup while Hadoop 2.x usually takes a min or two."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Hadoop Operations"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Running Hadoop Jobs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Three basic ways of runing MapReduce jobs:\n",
      "\n",
      "1. __Java MapReduce__ MapReduce Job written in Java\n",
      "    - Most efficient method\n",
      "2. Running __Hadoop Streaming__ with other languages: C++, Python, etc.\n",
      "    - Easiest to use but least efficient interface\n",
      "3. __Hadoop Pipes__: C++ interface using sockets between Java and C++"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Java MapReduce"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hadoop is written in Java code, thus one of the main ways of running MapReduce jobs is with Java.\n",
      "```\n",
      "$ hadoop jar <java file> [mainClass] args...\n",
      "```"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Hadoop Streaming"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hadoop Streaming allows Hadoop to use any programming languages with stdin, stdout: C++, Python, Perl, etc.\n",
      "```\n",
      "$ hadoop jar ${HADOOP_HOME}/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
      "\t-input <InputDirs> \\     \n",
      "\t-output <OutputDir> \\\n",
      "\t[-file <MapperFileToBeSent>] \\ \n",
      "\t-mapper <MapperFile> \\\n",
      "\t[-file <ReducerFileToBeSent>] \\ \n",
      "\t-reducer <ReducerFile>\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Hadoop Cheatsheet gives an outline of how to use this\n",
      "    - Remember any files to be used must be sent using -file option (the mapper and reducer nodes need a copy of the files)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Hadoop Streaming: Ver 1.x vs 2.x   "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hadoop Streaming for version 1.x can only use text for stdin and stdout, while 2.x is not.\n",
      "\n",
      "__1.x__ This means that stdin takes in text and then break the text back into the datatypes needed for computations.stdout is a print statement with a tab character seperating keys (\\t)\n",
      "\n",
      "__2.x__ Can do text as well as stream objects\n",
      "- Python would use emit() function to pass along each key-value pair instead of printing like in 1.x"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Hadoop Pipes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can connect to Hadoop C++ via pipes operator. Pipes makes a socket connect from Java to C++.  \n",
      "\n",
      "We can run a pipes job by the command:  \n",
      "```\n",
      "hadoop pipes \\\n",
      "    -input inputPath \\\n",
      "    -output outputPath \\\n",
      "    -program path/to/C++/executable\n",
      "```"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "MapReduce Web UI"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://localhost:50070/dfshealth.jsp"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HTML('<iframe src=http://localhost:50070/dfshealth.jsp width=1000 height=600></iframe>')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "html": [
        "<iframe src=http://localhost:50070/dfshealth.jsp width=1000 height=600></iframe>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "<IPython.core.display.HTML at 0x1038798d0>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "http://localhost:8088/cluster"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HTML('<iframe src=http://localhost:8088/cluster width=1000 height=600></iframe>')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "html": [
        "<iframe src=http://localhost:8088/cluster width=1000 height=600></iframe>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "<IPython.core.display.HTML at 0x103879910>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Hadoop Commands"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[HDFS Commands](http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-common/CommandsManual.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HTML('<iframe src=http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-common/CommandsManual.html width=1000 height=600></iframe>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<iframe src=http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-common/CommandsManual.html width=1000 height=600></iframe>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "<IPython.core.display.HTML at 0x109e74f10>"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "[Yarn Commands](http://hadoop.apache.org/docs/r2.5.2/hadoop-yarn/hadoop-yarn-site/YarnCommands.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HTML('<iframe src=http://hadoop.apache.org/docs/r2.5.2/hadoop-yarn/hadoop-yarn-site/YarnCommands.html width=1000 height=600></iframe>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<iframe src=http://hadoop.apache.org/docs/r2.5.2/hadoop-yarn/hadoop-yarn-site/YarnCommands.html width=1000 height=600></iframe>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "<IPython.core.display.HTML at 0x109e74ed0>"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Common HDFS File Commands"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Commands for HDFS is very similar to bash\n",
      "```\n",
      "$ hdfs [--config confdir] dfs <Cmd> <args>\n",
      "```\n",
      "| Cmd | args | | Description | args Description |\n",
      "|-----|------|-|-------------|------------------|\n",
      "| `-ls` | `-R` | path(s) | List files in path(s) | `-R` =including subdirs |\n",
      "| `-cat` |  | path(s) | Print files in path(s) |  |\n",
      "| `-mkdir` | `-p` | path | Create directory | `-p` =including parents  |\n",
      "| `-rm` | `-r` | file(s)/dir | Remove files | `-r` =including subdirectories |\n",
      "| `-put` |  | localFiles HDFSdir | Copy localFiles to HDFSdir |  |"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "| Cmd | args | | Description | args Description |\n",
      "|-----|------|-|-------------|------------------|\n",
      "| `-get` |  | HDFSfiles localDir | Copy HDFSfiles to localDest |  |\n",
      "| `-getmerge` |  | HDFSfiles localDir | Concatenate HDFSfiles to localDest |  |  |\n",
      "- More commands are on the cheatsheet attached"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "HDFS File Command Examples"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```\n",
      "$ hdfs dfs -ls \n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "! hdfs dfs -ls "
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "15/02/28 14:02:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found 2 items\r\n",
        "drwxr-xr-x   - darrellaucoin supergroup          0 2015-01-20 16:33 input\r\n",
        "drwxr-xr-x   - darrellaucoin supergroup          0 2015-02-15 16:31 out\r\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "```\n",
      "$ echo \"Hello world!\" > ~/example.txt\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "```\n",
      "$ hdfs dfs -put ~/example.txt /user/darrellaucoin\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "! echo \"Hello world!\" > ~/example.txt\n",
      "! hdfs dfs -put ~/example.txt /user/darrellaucoin"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "15/02/28 14:02:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "```\n",
      "$ hdfs dfs -ls \n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "! hdfs dfs -ls "
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "15/02/28 14:02:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found 3 items\r\n",
        "-rw-r--r--   1 darrellaucoin supergroup         13 2015-02-28 14:02 example.txt\r\n",
        "drwxr-xr-x   - darrellaucoin supergroup          0 2015-01-20 16:33 input\r\n",
        "drwxr-xr-x   - darrellaucoin supergroup          0 2015-02-15 16:31 out\r\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "```\n",
      "$ hdfs dfs -cat example.txt\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "! hdfs dfs -cat example.txt"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "15/02/28 14:02:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Hello world!\r\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "```\n",
      "$ hdfs dfs -rm example.txt\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "! hdfs dfs -rm example.txt"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "15/02/28 14:02:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "15/02/28 14:03:00 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
        "Deleted example.txt\r\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Hadoop Streaming Command"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```\n",
      "$ hadoop [--config confdir] jar ${HADOOP_HOME}/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
      "\t[genericOptions] <streamingOptions>\n",
      "```\n",
      "####streamingOptions\n",
      "\n",
      "| Cmd | | Description |\n",
      "|-----|-|-------------|\n",
      "| `-input` | dir/filename | Input location |\n",
      "| `-output` | dir | Output location |\n",
      "| `-mapper` | executable | Mapper executable |\n",
      "| `-reducer` | executable | Reducer executable |\n",
      "| `-file` | filename | File that needs to be sent to clusters (Mappers, reducer, combiners and other files they need) |"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "```\n",
      "$ hadoop [--config confdir] jar ${HADOOP_HOME}/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
      "\t[genericOptions] <streamingOptions>\n",
      "```\n",
      "\n",
      "| Cmd | | Description |\n",
      "|-----|-|-------------|\n",
      "| `-combiner` | executable | Combiner executable |\n",
      "| `-inputformat` | JavaClassName | Java Class specifying format of key-value pairs of text class for input. TextInputFormat is default |\n",
      "| `-outputformat` | JavaClassName | Java Class specifying format of key-value pairs of text class for output. TextOutputFormat is default |\n",
      "| `-numReduceTasks` | integer | The number of reducers to use |"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "```\n",
      "$ hadoop [--config confdir] jar ${HADOOP_HOME}/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
      "\t[genericOptions] <streamingOptions>\n",
      "```\n",
      "- Be sure to place the generic options before the streaming options, otherwise the command will fail.  \n",
      "\n",
      "####Some genericOptions\n",
      "\n",
      "| Parameter | | Description |\n",
      "|-----------|-|-------------|\n",
      "| `-D` | property=value | Set hadoop property to value |\n",
      "| `-files` | file1, file2, ... | Comma-separated files to be copied to the Map/Reduce cluster |"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "MapReduce Properties"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hadoop is __highly__ configurable, both on the admin and MapReduce job side. \n",
      "- Most options are for performance tuning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Map-Side Tuning Properties\n",
      "\n",
      "| Property | Type | Default | Description |\n",
      "|----------|------|---------|-------------|\n",
      "| mapreduce.compress.map.output | bool | false | Compress map outputs? |\n",
      "| mapreduce.map.output.compression.codec | Class name | org.apache.hadoop.io.compress.DefaultCodec | Compression codec for map output |\n",
      "| io.sort.mb | int | 100 | Memory buffer size (Mb) while sorting map output |\n",
      "| io.sort.spill.percent | float | 0.80 | Threshold usage for map output memory buffer and record boundaries index to start spilling to disk |\n",
      "| io.sort.factor | int | 10 | The max number of streams to  |\n",
      "| mapreduce.input.lineinputformat.linespermap | int | 1 | Used with NLineInputFormat, Defines input splits with a number of lines  | \n",
      "|  |  |  |  |\n",
      "-inputformat org.apache.hadoop.mapred.lib.NLineInputFormat\n",
      "\n",
      "Reduce-Side Tuning Prorperties\n",
      "\n",
      "| Property | Type | Default | Description |\n",
      "|----------|------|---------|-------------|\n",
      "|  |  |  |  |\n",
      "|  |  |  |  |\n",
      "|  |  |  |  |\n",
      "|  |  |  |  |\n",
      "|  |  |  |  |\n",
      "|  |  |  |  |\n",
      "|  |  |  |  |\n",
      "\n",
      "Task Envonment Properties\n",
      "\n",
      "| Property | Type | Description | Example |\n",
      "|----------|------|-------------|---------|\n",
      "| mapreduce.job.id | string | Job ID | |\n",
      "| mapreduce.tip.id | string | Task ID |  |\n",
      "| mapreduce.task.id | string | Task attempt ID |  |\n",
      "| mapreduce.task.partition | int | Index of task in the job |  |\n",
      "| mapreduce.task.is.map | boolean | Is task a map task? |  |\n",
      "|  |  |  |  |\n",
      "|  |  |  |  |\n",
      "|  |  |  |  |\n",
      "\n",
      "Streaming Environment Properties\n",
      "\n",
      "Hadoop sets job configuration parameters as environment variables for Streaming programs. However, it replaces nonalphanumeric characters with underscores to make sure they are valid names. The following Python expression illustrates how you can retrieve the value of the mapred.job.id property from within a Python Streaming script:\n",
      "\n",
      "os.environ[\"mapred_job_id\"]\n",
      "\n",
      "Task JVM Reuse Properties\n",
      "\n",
      "| Property | Type | Description | Example |\n",
      "|----------|------|-------------|---------|\n",
      "|  |  |  |  |\n",
      "|  |  |  |  |\n",
      "|  |  |  |  |\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Debugging Hadoop"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hadoop can run in 3 different modes, 2 of which are are useful for debugging:\n",
      "1. __Local__: Hadoop installed an a single machine, everything is run in a single JVM instance, thus making tracking down problems easier.\n",
      "2. __Pseudo-Distribution__: Essentially a 1 node cluster. Useful for finding problems related to the distributed nature of Hadoop.\n",
      "3. __Full-Distribution__: A hadoop system installed on a cluster."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Counters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hadoop has various counters associated with a job (i.e. how many map, reduce tasks have run etc.).  \n",
      "\n",
      "Counters have the dual purpose of keeping the system informed that a task is still running as well as metrics on a jobs progress.  Custom counters can also be written in Java.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Built-in Counters\n",
      "For Hadoop 2.x\n",
      "\n",
      "| Group | Name/Enum |\n",
      "|-------|-----------|\n",
      "| MapReduce Task Counters | org.apache.hadoop.mapreduce.TaskCounter |\n",
      "| Job Counters | org.apache.hadoop.mapreduce.JobCounter |\n",
      "| Filesystem Counters | org.apache.hadoop.mapreduce.FileSystemCounter |\n",
      "| File Input Format Counters | org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter | \n",
      "| File Output Format Counters | org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter |"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "#### Task Counters\n",
      "\n",
      "| Counter | Description |\n",
      "|---------|-------------|\n",
      "| MAP_INPUT_RECORDS | Number of input records consumed |\n",
      "| COMBINE_INPUT_RECORDS | Number of input records consumed by combiners |\n",
      "| REDUCE_INPUT_GROUPS | Number of distinct key groups consumed by reducers |\n",
      "| REDUCE_INPUT_RECORDS | Number of input records consumed by reducers |\n",
      "| MAP_OUTPUT_RECORDS | Number of output records by mappers |\n",
      "| COMBINE_OUTPUT_RECORDS | Number of output records produced by combiners |\n",
      "| REDUCE_OUTPUT_RECORDS | Number of output records produced by reducers |"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "| Counter | Description |\n",
      "|---------|-------------|\n",
      "| MAP_INPUT_RECORDS | Number of input records consumed |\n",
      "| COMBINE_INPUT_RECORDS | Number of input records consumed by combiners |\n",
      "| REDUCE_INPUT_GROUPS | Number of distinct key groups consumed by reducers |\n",
      "| REDUCE_INPUT_RECORDS | Number of input records consumed by reducers |\n",
      "| MAP_OUTPUT_RECORDS | Number of output records by mappers |\n",
      "| COMBINE_OUTPUT_RECORDS | Number of output records produced by combiners |\n",
      "| REDUCE_OUTPUT_RECORDS | Number of output records produced by reducers |\n",
      "| MAP_SKIPPED_RECORDS | Number of skipped input records |\n",
      "| MAP_INPUT_BYTES | Number of uncompressed bytes consumed |\n",
      "| MAP_OUTPUT_BYTES | Number of uncompressed bytes by mappers |\n",
      "| MAP_OUTPUT_MATERIALIZED_BYTES | Number of bytes by mappers to HD |\n",
      "| REDUCE_SKIPPED_GROUPS | Number of distinct keys skipped by reducers |\n",
      "| REDUCE_SKIPPED_RECORDS | Number of input records skipped by reducers |\n",
      "| REDUCE_SHUFFLE_BYTES | Number of bytes of map output copied by shuffle to reducers |\n",
      "| SPILLED_RECORDS | Number of records spilled to disk by mappers and reducers |\n",
      "| CPU_MILLISECONDS | Cumulative CPU time for a task in miliseconds |\n",
      "| PHYSICAL_MEMORY_BYTES | The physical memory being used by a task in bytes |\n",
      "| VIRTUAL_MEMORY_BYTES | The virtual memory being used by a task in bytes |\n",
      "| COMMITTED_HEAP_BYTES | Total amount of memory available in the JVM in bytes |\n",
      "| SHUFFLED_MAPS | Number of files transferred from mappers to reducers by shuffle  |\n",
      "| FAILED_SHUFFLE | Number of failed file transfers to reducers by shuffle |\n",
      "| MERGED_MAP_OUTPUTS | Number of map outputs that have been merged in reducer |\n",
      "| BYTES_READ | Number of bytes read by mapper and reducers |\n",
      "| BYTES_WRITTEN | Number of bytes written by mappers and reducers |\n",
      "| SPLIT_RAW_BYTES | Bytes of input-split objects read by mappers. This is split metadata. |\n",
      "| GC_TIME_MILLIS | Milliseconds for garbage collection in tasks |"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "#### File Input Format Counters\n",
      "| Counter | Description |\n",
      "|---------|-------------|\n",
      "| BYTES_READ | Bytes read by mappers |"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "#### File Output Format \n",
      "\n",
      "| Counter | Description |\n",
      "|---------|-------------|\n",
      "| BYTES_WRITTEN | Bytes written reducers or (for map-only jobs) mappers |"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "#### Job Counters\n",
      "\n",
      "| Counter | Description |\n",
      "|---------|-------------|\n",
      "| TOTAL_LAUNCHED_MAPS | Number of map launched tasks |\n",
      "| TOTAL_LAUNCHED_REDUCES | Number of launched reduce tasks |\n",
      "| TOTAL_LAUNCHED_UBERTASKS | Number of uber tasks |\n",
      "| NUM_UBER_SUBMAPS | Number of maps in uber tasks |\n",
      "| NUM_UBER_SUBREDUCES | Number of reduces in uber tasks |\n",
      "| NUM_FAILED_MAPS | Number of failed maps |\n",
      "| NUM_FAILED_REDUCES | Number of failed reduces |\n",
      "| NUM_FAILED_UBERTASKS | Number of failed uber tasks |\n",
      "| DATA_LOCAL_MAPS | Number of data local mappers |\n",
      "| RACK_LOCAL_MAPS | Number of rack local mappers |\n",
      "| OTHER_LOCAL_MAPS | Number of non-local rack mappers |\n",
      "| SLOTS_MILLIS_MAPS | Total time running mappers (milliseconds) |\n",
      "| SLOTS_MILLIS_REDUCES | Total time running reducers (milliseconds) |"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Hadoop Streaming and R"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Setting Environment Settings for R\n",
      "\n",
      "Either run the following code in R, each time you need to do a Hadoop job:  \n",
      "\n",
      "```\n",
      "#The Hadoop installation directory\n",
      "Sys.setenv(\"HADOOP_PREFIX\"=\"/usr/local/hadoop/2.*\")\n",
      "#The location for Hadoop executable\n",
      "Sys.setenv(\"HADOOP_CMD\"=\"/usr/local/hadoop/2.*/bin/hadoop\")\n",
      "#The location of hadoop streaming jar file\n",
      "Sys.setenv(\"HADOOP_STREAMING\"=\"/usr/local/hadoop/2.*/share/hadoop/tools/lib/hadoop-streaming-2.*.jar\") \n",
      "```\n",
      "\n",
      "or include the following into ~/.bashrc file\n",
      "\n",
      "```\n",
      "export HADOOP_PREFIX=${HADOOP_HOME}\n",
      "export HADOOP_CMD=${HADOOP_HOME}/bin/hadoop \n",
      "export HADOOP_STREAMING=${HADOOP_HOME}/share/hadoop/tools/lib/hadoop-streaming-*.jar\n",
      "```"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "rmr2: mapreduce"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__mapreduce__ Defines and executes a map reduce job. (Pkg rmr2)\n",
      "```\n",
      "mapreduce(input, output = NULL, map = to.map(identity), \n",
      "    reduce = NULL, combine = NULL, \n",
      "    input.format = \"native\",   \n",
      "    output.format = \"native\", \n",
      "    vectorized.reduce = FALSE,  \n",
      "    in.memory.combine = FALSE,\n",
      "    backend.parameters = list(),\n",
      "    verbose = TRUE) \n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- __input__ Can be:\n",
      "    - A set of file paths in HDFS\n",
      "    - A __Big Data Object__ (a stub of information on some data in HDFS)\n",
      "    - A _list_ of a combination of both"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- __output__ A path to the destination folder on HDFS; if missing, a Big Data Object is returned."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "- __map__ An optional R map function that returns either NULL or a keyval object "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- __reduce__ An optional R reduce function that returns either NULL or a keyval object "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- __combine__ refers to:\n",
      "\n",
      "    - A function with the same signature and possible return values as the reduce function, or \n",
      "\n",
      "    - TRUE, which means use the reduce function as combiner.\n",
      "\n",
      "    - NULL means no combiner is used."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "- __input.format__ Input format specification, see make.input.format  \n",
      "$$\\mbox{input.format}=\\begin{cases}\n",
      "\\mbox{\"text\"} & \\mbox{Plain text}\\\\\n",
      "\\mbox{\"json\"} & \\mbox{JavaScript Object Notation}\\\\\n",
      "\\mbox{\"csv\"} & \\mbox{Comma Separated Values}\\\\\n",
      "\\mbox{\"native\"} & \\mbox{R saved datatypes}\\\\\n",
      "\\mbox{\"sequence.typedbytes\"}\\\\\n",
      "\\mbox{\"hbase\"} & \\mbox{hbase table format}\\\\\n",
      "\\mbox{\"pig.hive\"} & \\mbox{pig/hive table format}\n",
      "\\end{cases}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "- __output.format__ Output format specification, see make.output.format  \n",
      "$$\\mbox{output.format}=\\begin{cases}\n",
      "\\mbox{\"text\"} & \\mbox{Plain text}\\\\\n",
      "\\mbox{\"json\"} & \\mbox{JavaScript Object Notation}\\\\\n",
      "\\mbox{\"csv\"} & \\mbox{Comma Separated Values}\\\\\n",
      "\\mbox{\"native\"} & \\mbox{R saved datatypes}\\\\\n",
      "\\mbox{\"sequence.typedbytes\"}\\\\\n",
      "\\mbox{\"pig.hive\"} & \\mbox{pig/hive table format}\n",
      "\\end{cases}$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "rmr2: keyval"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "- __keyval(key,\u00a0val)__ Function that returns key-value pair, only val can ever be NULL. This is what mapper, reducer and combiner functions return."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "rmr2: from.dfs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "__from.dfs__ Read R objects from HDFS.\n",
      "```\n",
      "from.dfs(input, format = \"native\")\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "- __input__ A valid path to HDFS or a big.data.object\n",
      "\n",
      "- __format__ Either\n",
      "\n",
      "    - A string naming the format, or\n",
      "\n",
      "    - A value returned by the function make.input.format"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "rmr2: to.dfs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "__to.dfs__ Write R objects to HDFS and return a Big Data Object. (Useful for test purposes only)\n",
      "```\n",
      "to.dfs(kv, output = dfs.tempfile(), format = \"native\")\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "- __input__ A valid path to HDFS or a big.data.object\n",
      "\n",
      "- __format__ Either\n",
      "\n",
      "    - A string naming the format, or\n",
      "\n",
      "    - A value returned by the function make.input.format"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "rmr2: keys( ) and values( )"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__keys__ Get keys from a big data object or HDFS file path\n",
      "```\n",
      "keys(kv)\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "__values__ Get values from a big data object or HDFS file path\n",
      "\n",
      "```\n",
      "values(kv)\n",
      "```"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "rmr2: Preamble"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "For rmr2 to work, some variables need to be set in bash:\n",
      "```\n",
      "Sys.setenv(\"HADOOP_PREFIX\"=\"/usr/local/hadoop/2.2.0\")\n",
      "Sys.setenv(\"HADOOP_CMD\"=\"/usr/local/hadoop/2.2.0/bin/hadoop\")\n",
      "Sys.setenv(\"HADOOP_STREAMING\"=\n",
      "    \"/usr/local/hadoop/2.2.0/share/hadoop/tools/lib/hadoop-streaming-2.2.0.jar\")\n",
      "```"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "Some Useful R Functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "### system\n",
      "__system__ Invokes OS command specified by the character string command. (Pkg base)\n",
      "```\n",
      "system(command, intern = FALSE,\n",
      "       ignore.stdout = FALSE, ignore.stderr = FALSE,\n",
      "       wait = TRUE, input = NULL, show.output.on.console = TRUE,\n",
      "       minimized = FALSE, invisible = TRUE) \n",
      "```  \n",
      "\n",
      "- __command__ Character string to be invoked in terminal.\n",
      "\n",
      "- __intern__ a logical (not NA) which indicates whether to capture the output of the command as an R character vector.\n",
      "\n",
      "- __ignore.stdout, ignore.stderr__ a logical (not NA) indicating whether messages written to \u2018stdout\u2019 or \u2018stderr\u2019 should be ignored.\n",
      "\n",
      "- __wait__ a logical (not NA) indicating whether the R interpreter should wait for the command to finish, or run it asynchronously. This will be ignored (and the interpreter will always wait) if intern = TRUE.\n",
      "\n",
      "- __input__ if a character vector is supplied, this is copied one string per line to a temporary file, and the standard input of command is redirected to the file.\n",
      "\n",
      "- __show.output.on.console__, minimized, invisible arguments that are accepted on Windows but ignored on this platform, with a warning.  \n",
      "\n",
      "__Example:__ Command for hadoop to remove previous contents of out\n",
      "```\n",
      "system(\"${HADOOP_CMD} fs -rm -r /user/darrellaucoin/out\")\n",
      "```\n",
      "---\n",
      "### file.path\n",
      "__file.path__ Construct a path from components to be easily translated accross different OS (Mac, Windows, Linux). (Pkg base)\n",
      "```\n",
      "file.path(...)\n",
      "```  \n",
      "\n",
      "- __...__ character vectors \n",
      "\n",
      "__Example:__ Command for hadoop to remove previous contents of out\n",
      "```\n",
      "hdfs.root <-'/user/darrellaucoin'\n",
      "input <- file.path(hdfs.root, 'input') \n",
      "```\n",
      "---\n",
      "### strsplit\n",
      "__strsplit__ Split elements of a character vector x into substrings, split with matches to the regular expression in split argument. (Pkg base)\n",
      "```\n",
      "strsplit(x, split, fixed = FALSE, perl = FALSE, useBytes = FALSE) \n",
      "```\n",
      "\n",
      "- __x__ character vector \n",
      "\n",
      "- __split__ character vector containing regular expressions to tell when to break the substring. If split has length 0, x is split to individual characters.\n",
      "\n",
      "- __perl__ logical. Should perl-compatible regular expressions be used?\n",
      "\n",
      "- __useBytes__ logical. Should matching be done byte by byte?  \n",
      "\n",
      "__Example:__ \n",
      "```\n",
      "str = 'this is a string' \n",
      "strsplit(str, split = ' ')\n",
      "```\n",
      "---\n",
      "### list\n",
      "__list__ construct a list from the objects. A list is like a vector that can have different datatypes.\n",
      "```\n",
      "list(x, all.names = FALSE)\n",
      "```\n",
      "\n",
      "- __...__ objects to coerced into a list\n",
      "\n",
      "- __x__ object to coerced into a list\n",
      "\n",
      "- __all.names__ logical. whether to copy all values or (default) only those whose names do not begin with a dot.  \n",
      "\n",
      "__Example:__ \n",
      "```\n",
      "a.list = list(1,2,3,'string', 2.23, lm(y~x))\n",
      "```\n",
      "---\n",
      "### unlist\n",
      "__unlist__ Simplifies a list structure x to a vector containing atomic components which occur in x. (Pkg base)\n",
      "```\n",
      "unlist(x, recursive = TRUE, use.names = TRUE)\n",
      "```\n",
      "\n",
      "- __x__ a list or vector\n",
      "\n",
      "- __recurvisve__ logical\n",
      "\n",
      "- __use.names__ logical. Should names be preserved?\n",
      "__Example:__ \n",
      "```\n",
      "vec = unlist(a.list)\n",
      "```\n",
      "---\n",
      "### write\n",
      "__write__ \n",
      "```\n",
      "write(x, file = \"data\",\n",
      "      ncolumns = if(is.character(x)) 1 else 5,\n",
      "      append = FALSE, sep = \" \")\n",
      "```\n",
      "\n",
      "- __x__ the data to be written\n",
      "\n",
      "- __file__ connection or character string naming the file to written to. \n",
      "    - For Hadoop, we want to write to standard error (`stderr()`) for it to go to the client.\n",
      "\n",
      "- __ncolumns__ number of columns to write the data in\n",
      "\n",
      "- __append__ if TRUE the data are appended to the connection.\n",
      "\n",
      "- __sep__ string used to serparate the columns  \n",
      "\n",
      "__Example:__ \n",
      "```\n",
      "write(\"prints to stderr\", stderr())\n",
      "```"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Debugging Hadoop within R"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "Debugging is essential to running a MapReduce job and ensuring everything is working correctly. As such, we have two functions to help us debugg code:  \n",
      "\n",
      "1. __rmr.options__ when we are running on hadoop (either in pseudo-distribution or full-distribution mode). When in hadoop mode, there are several options to collect information to help debug.  \n",
      "\n",
      "    - When trying to look for bugs it is usually best look in local mode to find potential problems within R, then failing that look in pseudo-distributed mode for any problems (connections problems between R and Hadoop etc.) Profiling works by writing out the call stack every interval seconds, to the file specified  \n",
      "\n",
      "2. __Rprof__ when we are running on local mode (using `rmr.options(backend = \"local\")` ). When local mode is turned on, hadoop is never run but a simulation of hadoop entirely within R is run, as such Rprof is used to collect any debugging information we might need.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "__rmr.options__ Set and get package options. (Pkg rmr2)\n",
      "```\n",
      "rmr.options(backend = c(\"hadoop\", \"local\"),\n",
      "\tprofile.nodes = c(\"off\", \"calls\", \"memory\", \"both\"),\n",
      "\thdfs.tempdir = \"/tmp\",\n",
      "\texclude.objects = NULL,\n",
      "\tbackend.parameters = list())\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- __...__ Names of options to get values of, as length of one character vectors.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- __backend__ Use either hadoop, or the current R interpreter, sequentially, for learning and debugging.  \n",
      "$$=\\begin{cases}\n",
      "\\mbox{\"local\"} & \\mbox{implemented in R (helpful for debugging)}\\\\\n",
      "\\mbox{\"hadoop\"} & \\mbox{implemented in Hadoop}\n",
      "\\end{cases}$$  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "- __profile.nodes__ Collect profiling and memory information when running additional R interpreters (besides the current one) on the cluster. No effect on the local backend, use Rprof instead. For backward compatibility, \"calls\" is equivalent to TRUE and \"off\" to FALSE  \n",
      "$$\\mbox{profile.nodes}=\\begin{cases}\n",
      "\\mbox{\"off\"} & \\mbox{No collection}\\\\\n",
      "\\mbox{\"calls\"} & \\mbox{Collect R calls}\\\\\n",
      "\\mbox{\"memory\"} & \\mbox{Collect memory information}\\\\\n",
      "\\mbox{\"both\"} & \\mbox{Collect both}\n",
      "\\end{cases}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "__Rprof__ when we are running on local mode (using `rmr.options(backend = \"local\")` ). When local mode is turned on, hadoop is never run but a simulation of hadoop entirely within R is run, as such Rprof is used to collect any debugging information we might need.  \n",
      "`summaryRprof`  \n",
      "- Functions will only be recorded in the profile log if they put a context on the call stack\n",
      "\n",
      "- Individual statements will be recorded in the profile log if line.profiling is set to TRUE, and if the code being executed was parsed with source references."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Hadoop Examples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext rpy2.ipython"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The rpy2.ipython extension is already loaded. To reload it, use:\n",
        "  %reload_ext rpy2.ipython\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Word Count"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want a word count in a text (or group of texts)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "```\n",
      "Sys.setenv(\"HADOOP_PREFIX\"=\"/usr/local/hadoop/2.5.2\")\n",
      "Sys.setenv(\"HADOOP_CMD\"=\"/usr/local/hadoop/2.5.2/bin/hadoop\")\n",
      "Sys.setenv(\"HADOOP_STREAMING\"=\n",
      "    \"/usr/local/hadoop/2.5.2/share/hadoop/tools/lib/hadoop-streaming-2.5.2.jar\")\n",
      "library(rmr2)\n",
      "library(data.table)\n",
      "system('${HADOOP_CMD} fs -rm -r /user/darrellaucoin/out/')\n",
      "map=function(k,lines) {\n",
      "    words.list = strsplit(lines, ' ') \n",
      "    words = unlist(words.list)\n",
      "    return( keyval(words, 1) ) \n",
      "}\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "```\n",
      "reduce=function(word, counts) {\n",
      "    keyval(word, sum(counts)) \n",
      "}\n",
      "mapreduce(input='/user/darrellaucoin/input/War_and_Peace.txt',\n",
      "    output='/user/darrellaucoin/out',\n",
      "    input.format=\"text\",\n",
      "    map=map,\n",
      "    reduce=reduce\n",
      ")\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "Sys.setenv(\"HADOOP_PREFIX\"=\"/usr/local/hadoop/2.5.2\")\n",
      "Sys.setenv(\"HADOOP_CMD\"=\"/usr/local/hadoop/2.5.2/bin/hadoop\")\n",
      "Sys.setenv(\"HADOOP_STREAMING\"=\n",
      "    \"/usr/local/hadoop/2.5.2/share/hadoop/tools/lib/hadoop-streaming-2.5.2.jar\")\n",
      "library(rmr2)\n",
      "library(data.table)\n",
      "system('${HADOOP_CMD} fs -rm -r /user/darrellaucoin/out/')\n",
      "\n",
      "map=function(k,lines) {\n",
      "    words.list = strsplit(lines, ' ') \n",
      "    words = unlist(words.list)\n",
      "    return( keyval(words, 1) ) \n",
      "}\n",
      "reduce=function(word, counts) {\n",
      "    keyval(word, sum(counts)) \n",
      "}\n",
      "mapreduce(input='/user/darrellaucoin/input/War_and_Peace.txt',\n",
      "    output='/user/darrellaucoin/out',\n",
      "    input.format=\"text\",\n",
      "    map=map,\n",
      "    reduce=reduce\n",
      ")"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "[1] \"/user/darrellaucoin/out\"\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "```\n",
      "count = from.dfs('/user/darrellaucoin/out')\n",
      "results = as.data.table(count)\n",
      "setnames(results, c('word', 'count')) \n",
      "results[order(results$count, decreasing=T), ]\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "count = from.dfs('/user/darrellaucoin/out')\n",
      "results = as.data.table(count)\n",
      "setnames(results, c('word', 'count')) \n",
      "results[order(results$count, decreasing=T), ]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "                                       word count\n",
        "    1:                                  the 31708\n",
        "    2:                                  and 20570\n",
        "    3:                                   to 16322\n",
        "    4:                                   of 14854\n",
        "    5:                                    a 10040\n",
        "   ---                                           \n",
        "42062:         insignificance--particularly     1\n",
        "42063:        http://pglaf.org/fundraising.     1\n",
        "42064:       http://gutenberg.org/license).     1\n",
        "42065:      Hofs-kriegs-wurst-schnapps-Rath     1\n",
        "42066: http://www.gutenberg.org/2/6/0/2600/     1\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Linear Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "To understand the MapReduce framework, lets solve a familar problem of Linear Regression. For Hadoop/MapReduce to work we MUST figure out how to parallelize our code, in other words how to use the hadoop system to only need to make a subset of our calculations on a subset of our data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Assumption__: The value of p, the number of explanatory variables is small enough for R to easily handle i.e.  \n",
      "$$n\\gg p$$  \n",
      "\n",
      "We know from linear regression, that our estimate of $\\hat{\\beta}$:  \n",
      "$$X^{T}X\\hat{\\beta}=X^{T}y$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "$\\left(X^{T}X\\right)_{p\\times p}$ and $\\left(X^{T}y\\right)_{p\\times1}$ is small enough for R to solve for $\\hat{\\beta}$, thus we only need $X^{T}X,X^{T}y$ to get $\\hat{\\beta}$.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "To break up this calculation we break our matrix $X$ into submatricies $X_{i}$:  \n",
      "$$X=\\begin{bmatrix}X_{1}\\\\\n",
      "X_{2}\\\\\n",
      "X_{3}\\\\\n",
      "\\vdots\\\\\n",
      "X_{n}\n",
      "\\end{bmatrix}\t\ty=\\begin{bmatrix}y_{1}\\\\\n",
      "y_{2}\\\\\n",
      "y_{3}\\\\\n",
      "\\vdots\\\\\n",
      "y_{n}\n",
      "\\end{bmatrix}$$  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "$$X^{T}X\t=\t\\begin{bmatrix}X_{1}^{T} & X_{2}^{T} & X_{3}^{T} & \\cdots & X_{n}^{T}\\end{bmatrix}\\begin{bmatrix}X_{1}\\\\\n",
      "X_{2}\\\\\n",
      "X_{3}\\\\\n",
      "\\vdots\\\\\n",
      "X_{n}\n",
      "\\end{bmatrix} \\implies$$\n",
      "\n",
      "$$X^{T}X\t=\t\\begin{bmatrix}X_{1}^{T}X_{1}+ & X_{2}^{T}X_{2}+ & X_{3}^{T}X_{3}+ & \\cdots & +X_{n}^{T}X_{n}\\end{bmatrix}$$  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "$$X^{T}y=\\begin{bmatrix}X_{1}^{T} & X_{2}^{T} & X_{3}^{T} & \\cdots & X_{n}^{T}\\end{bmatrix}\\begin{bmatrix}y_{1}\\\\\n",
      "y_{2}\\\\\n",
      "y_{3}\\\\\n",
      "\\vdots\\\\\n",
      "y_{n}\n",
      "\\end{bmatrix}\\implies$$\n",
      "\n",
      "$$ X^{T}y=\\begin{bmatrix}X_{1}^{T}y_{1}+ & X_{2}^{T}y_{2}+ & X_{3}^{T}y_{3}+ & \\cdots & +X_{n}^{T}y_{n}\\end{bmatrix}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "```\n",
      "Sys.setenv(\"HADOOP_PREFIX\"=\"/usr/local/hadoop/2.5.2\")\n",
      "Sys.setenv(\"HADOOP_CMD\"=\"/usr/local/hadoop/2.5.2/bin/hadoop\")\n",
      "Sys.setenv(\"HADOOP_STREAMING\"=\n",
      "    \"/usr/local/hadoop/2.5.2/share/hadoop/tools/lib/hadoop-streaming-2.5.2.jar\")\n",
      "library(rmr2)\n",
      "library(data.table)\n",
      "#Setup variables \n",
      "p = 10 \n",
      "num.obs = 200\n",
      "beta.true = 1:(p+1) \n",
      "X = cbind(rep(1,num.obs), matrix(rnorm(num.obs * p), \n",
      "    ncol = p))\n",
      "y = X %*% beta.true + rnorm(num.obs) \n",
      "X.index = to.dfs(cbind(y, X)) \n",
      "rm(X, y, num.obs, p) \n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "```\n",
      "map.XtX = function(., Xi) {\n",
      "    Xi = Xi[,-1] #Get rid of y values in Xi\n",
      "    keyval(1, list(t(Xi) %*% Xi)) \n",
      "}\n",
      "map.Xty = function(., Xi) {\n",
      "    yi = Xi[,1] # Retrieve the y values\n",
      "    Xi = Xi[,-1] #Get rid of y values in Xi\n",
      "    keyval(1, list(t(Xi) %*% yi)) \n",
      "}\n",
      "Sum = function(., YY) {\n",
      "    keyval(1, list(Reduce('+', YY))) \n",
      "}\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "Sys.setenv(\"HADOOP_PREFIX\"=\"/usr/local/hadoop/2.5.2\")\n",
      "Sys.setenv(\"HADOOP_CMD\"=\"/usr/local/hadoop/2.5.2/bin/hadoop\")\n",
      "Sys.setenv(\"HADOOP_STREAMING\"=\n",
      "    \"/usr/local/hadoop/2.5.2/share/hadoop/tools/lib/hadoop-streaming-2.5.2.jar\")\n",
      "library(rmr2)\n",
      "library(data.table)\n",
      "#Setup variables \n",
      "p = 10 \n",
      "num.obs = 200\n",
      "beta.true = 1:(p+1) \n",
      "X = cbind(rep(1,num.obs), matrix(rnorm(num.obs * p), \n",
      "    ncol = p))\n",
      "y = X %*% beta.true + rnorm(num.obs) \n",
      "X.index = to.dfs(cbind(y, X)) \n",
      "rm(X, y, num.obs, p) \n",
      "##########################\n",
      "map.XtX = function(., Xi) {\n",
      "    Xi = Xi[,-1] #Get rid of y values in Xi\n",
      "    keyval(1, list(t(Xi) %*% Xi)) \n",
      "}\n",
      "map.Xty = function(., Xi) {\n",
      "    yi = Xi[,1] # Retrieve the y values\n",
      "    Xi = Xi[,-1] #Get rid of y values in Xi\n",
      "    keyval(1, list(t(Xi) %*% yi)) \n",
      "}\n",
      "Sum = function(., YY) {\n",
      "    keyval(1, list(Reduce('+', YY))) \n",
      "}"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "- The reason we are returning a list in the map function is because otherwise Reduce will only return a some of the elements of the matricies. \n",
      "\n",
      "    - list prevents this by Reduce iterating though the elements of the list (the individual $X_{i}^{T}X_{i}$  matricies) and applying the binary function '+' to each one.\n",
      "\n",
      "    - list is used in the reduce function `Sum` because we will also use this as a combiner function and if we didn't use a list we would have the same problem as above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "```\n",
      "XtX = values(from.dfs(\n",
      "    mapreduce(input = X.index,\n",
      "    map = map.XtX,\n",
      "    reduce = Sum,\n",
      "    combine = TRUE)))[[1]]\n",
      "\n",
      "Xty = values(from.dfs(\n",
      "    mapreduce(\n",
      "    input = X.index,\n",
      "    map = map.Xty,\n",
      "    reduce = Sum,\n",
      "    combine = TRUE)))[[1]]\n",
      "beta.hat = solve(XtX, Xty)\n",
      "print(beta.hat)\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "XtX = values(from.dfs(\n",
      "    mapreduce(input = X.index,\n",
      "    map = map.XtX,\n",
      "    reduce = Sum,\n",
      "    combine = TRUE)))[[1]]\n",
      "\n",
      "Xty = values(from.dfs(\n",
      "    mapreduce(\n",
      "    input = X.index,\n",
      "    map = map.Xty,\n",
      "    reduce = Sum,\n",
      "    combine = TRUE)))[[1]]\n",
      "beta.hat = solve(XtX, Xty)\n",
      "print(beta.hat)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "           [,1]\n",
        " [1,]  1.045835\n",
        " [2,]  1.980511\n",
        " [3,]  2.993829\n",
        " [4,]  4.011599\n",
        " [5,]  5.074755\n",
        " [6,]  6.008534\n",
        " [7,]  6.947164\n",
        " [8,]  8.024570\n",
        " [9,]  9.024757\n",
        "[10,]  9.888609\n",
        "[11,] 10.893023\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "15/02/28 16:58:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable  \n",
      "packageJobJar: [/usr/local/hadoop/2.5.2/hdfs/tmp/hadoop-unjar2544978824408962002/] [] /var/folders/p3/jb65k2x55g1brywqlf6m10p00000gn/T/streamjob622039766772640798.jar tmpDir=null  \n",
      "15/02/28 16:58:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032  \n",
      "15/02/28 16:58:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032  \n",
      "15/02/28 16:58:21 INFO mapred.FileInputFormat: Total input paths to process : 1  \n",
      "15/02/28 16:58:21 INFO mapreduce.JobSubmitter: number of splits:2  \n",
      "15/02/28 16:58:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1425160610091_0002  \n",
      "15/02/28 16:58:22 INFO impl.YarnClientImpl: Submitted application application_1425160610091_0002  \n",
      "15/02/28 16:58:22 INFO mapreduce.Job: The url to track the job: http://v1020-wn-6-180.campus-dynamic.uwaterloo.ca:8088/proxy/application_1425160610091_0002/  \n",
      "15/02/28 16:58:22 INFO mapreduce.Job: Running job: job_1425160610091_0002  \n",
      "15/02/28 16:58:30 INFO mapreduce.Job: Job job_1425160610091_0002 running in uber mode : false  \n",
      "15/02/28 16:58:30 INFO mapreduce.Job:  map 0% reduce 0%  \n",
      "15/02/28 16:58:39 INFO mapreduce.Job:  map 50% reduce 0%  \n",
      "15/02/28 16:58:47 INFO mapreduce.Job:  map 100% reduce 0%  \n",
      "15/02/28 16:58:54 INFO mapreduce.Job:  map 100% reduce 100%  \n",
      "15/02/28 16:58:54 INFO mapreduce.Job: Job job_1425160610091_0002 completed successfully  \n",
      "15/02/28 16:58:54 INFO mapreduce.Job: Counters: 50  \n",
      "\tFile System Counters  \n",
      "\t\tFILE: Number of bytes read=413  \n",
      "\t\tFILE: Number of bytes written=311808  \n",
      "\t\tFILE: Number of read operations=0  \n",
      "\t\tFILE: Number of large read operations=0  \n",
      "\t\tFILE: Number of write operations=0  \n",
      "\t\tHDFS: Number of bytes read=26712  \n",
      "\t\tHDFS: Number of bytes written=546  \n",
      "\t\tHDFS: Number of read operations=13  \n",
      "\t\tHDFS: Number of large read operations=0  \n",
      "\t\tHDFS: Number of write operations=2  \n",
      "\tJob Counters   \n",
      "\t\tLaunched map tasks=2  \n",
      "\t\tLaunched reduce tasks=1  \n",
      "\t\tData-local map tasks=2  \n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=51344  \n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=20704  \n",
      "\t\tTotal time spent by all map tasks (ms)=12836  \n",
      "\t\tTotal time spent by all reduce tasks (ms)=5176  \n",
      "\t\tTotal vcore-seconds taken by all map tasks=12836  \n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5176  \n",
      "\t\tTotal megabyte-seconds taken by all map tasks=52576256  \n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=21200896  \n",
      "\tMap-Reduce Framework  \n",
      "\t\tMap input records=3  \n",
      "\t\tMap output records=3  \n",
      "\t\tMap output bytes=400  \n",
      "\t\tMap output materialized bytes=419  \n",
      "\t\tInput split bytes=186  \n",
      "\t\tCombine input records=3  \n",
      "\t\tCombine output records=3  \n",
      "\t\tReduce input groups=1  \n",
      "\t\tReduce shuffle bytes=419  \n",
      "\t\tReduce input records=3  \n",
      "\t\tReduce output records=3  \n",
      "\t\tSpilled Records=6  \n",
      "\t\tShuffled Maps =2  \n",
      "\t\tFailed Shuffles=0  \n",
      "\t\tMerged Map outputs=2  \n",
      "\t\tGC time elapsed (ms)=348  \n",
      "\t\tCPU time spent (ms)=0  \n",
      "\t\tPhysical memory (bytes) snapshot=0  \n",
      "\t\tVirtual memory (bytes) snapshot=0  \n",
      "\t\tTotal committed heap usage (bytes)=531628032  \n",
      "\tShuffle Errors  \n",
      "\t\tBAD_ID=0  \n",
      "\t\tCONNECTION=0  \n",
      "\t\tIO_ERROR=0  \n",
      "\t\tWRONG_LENGTH=0  \n",
      "\t\tWRONG_MAP=0  \n",
      "\t\tWRONG_REDUCE=0  \n",
      "\tFile Input Format Counters   \n",
      "\t\tBytes Read=26526  \n",
      "\tFile Output Format Counters   \n",
      "\t\tBytes Written=546  \n",
      "\trmr  \n",
      "\t\treduce calls=2  \n",
      "15/02/28 16:58:54 INFO streaming.StreamJob: Output directory: /tmp/file72e60803cf9  \n",
      "15/02/28 16:58:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable  \n",
      "15/02/28 16:58:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable  \n",
      "15/02/28 16:58:59 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.  \n",
      "Deleted /tmp/file72e243880be  \n",
      "15/02/28 16:59:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable  \n",
      "15/02/28 16:59:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable  \n",
      "15/02/28 16:59:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "__mapreduce.input.lineinputformat.linespermap__ When using NLineInputFormat, the number of lines of input data to include in each split.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Your Data isn't THAT BIG"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hadoop has several difficulties and limitations:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Paradigm shift of MapReduce: re-vision (and recoding) algorithms into the paradigm of MapReduce"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Debugging code is more difficult than just regular code"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Data security isn't as well defined as most SQL engines"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Alternatives"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Q:__ Is there a way of fitting the data in memory?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- A gigabyte csv file can be loaded into memory on most computers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "__Q:__ Can the process be done reading lines from a file?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- R and Python have functions like `readLines` that can read a file line by line"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "__Q:__ Can a SQL server handle the information/calculation?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- SQL doesn't require all data to be loaded into memory to execute it's queries\n",
      "    - SQL can handle queries over many Gigabytes of data "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- SQL tables can be indexed (unlike Hadoop) making for faster processing time  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- R packages like `dplyr` can translate _some_ R-code into SQL queries and can execute said code with a SQL backend\n",
      "    - Can also execute SQL queries from R, importing the results into a dataframe"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- PostgreSQL has an extension ([MADlib](http://madlib.net/)) allowing Machine Learning algorithms to run over large datasets on PostgreSQL"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "__Q:__ What if the data isn't structured for SQL tables? i.e. JSON file, binary blobs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- PostgreSQL does have a JSON datatype"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Can process the file using a script, reading line by line and then write to file as needed"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Questions?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sudo ipython nbconvert IntroHadoop.ipynb --to slides --post serve"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 15
    }
   ],
   "metadata": {}
  }
 ]
}